ID,URL,Date,Title,InfoTheory,CompVis,Math,Abstract
cs-9301111,arxiv.org/abs/cs/9301111,31/12/89,Nested satisfiability,0,0,0," Nested satisfiability A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way. "
cs-9301112,arxiv.org/abs/cs/9301112,31/3/90,A note on digitized angles,0,0,0, A note on digitized angles We study the configurations of pixels that occur when two digitized straight lines meet each other. 
cs-9301113,arxiv.org/abs/cs/9301113,31/7/91,Textbook examples of recursion,0,0,0," Textbook examples of recursion We discuss properties of recursive schemas related to McCarthy's ``91 function'' and to Takeuchi's triple recursion. Several theorems are proposed as interesting candidates for machine verification, and some intriguing open questions are raised. "
cs-9301114,arxiv.org/abs/cs/9301114,31/10/91,Theory and practice,0,0,0," Theory and practice The author argues to Silicon Valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical. He particularly considers the intersection of the theory of algorithms and practical software development. He combines examples from the development of the TeX typesetting system with clever jokes, criticisms, and encouragements. "
cs-9301115,arxiv.org/abs/cs/9301115,30/11/91,Context-free multilanguages,0,0,0," Context-free multilanguages This article is a sketch of ideas that were once intended to appear in the author's famous series, The Art of Computer Programming . He generalizes the notion of a context-free language from a set to a multiset of words over an alphabet. The idea is to keep track of the number of ways to parse a string. For example, fruit flies like a banana can famously be parsed in two ways; analogous examples in the setting of programming languages may yet be important in the future. The treatment is informal but essentially rigorous. "
cs-9301116,arxiv.org/abs/cs/9301116,30/6/92,The problem of compatible representatives,0,0,1, The problem of compatible representatives The purpose of this note is to attach a name to a natural class of combinatorial problems and to point out that this class includes many important special cases. We also show that a simple problem of placing nonoverlapping labels on a rectangular map is NP-complete. 
cs-9308102,arxiv.org/abs/cs/9308102,31/7/93,A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems,0,0,0," A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms. "
cs-9308101,arxiv.org/abs/cs/9308101,31/7/93,Dynamic Backtracking,0,0,0," Dynamic Backtracking Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches. "
cs-9309101,arxiv.org/abs/cs/9309101,31/8/93,An Empirical Analysis of Search in GSAT,0,0,0," An Empirical Analysis of Search in GSAT We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms. "
math-9310227,arxiv.org/abs/math/9310227,30/9/93,A linear construction for certain Kerdock and Preparata codes,1,0,1," A linear construction for certain Kerdock and Preparata codes The Nordstrom-Robinson, Kerdock, and (slightly modified) Pre\- parata codes are shown to be linear over , the integers . The Kerdock and Preparata codes are duals over , and the Nordstrom-Robinson code is self-dual. All these codes are just extended cyclic codes over . This provides a simple definition for these codes and explains why their Hamming weight distributions are dual to each other. First- and second-order Reed-Muller codes are also linear codes over , but Hamming codes in general are not, nor is the Golay code. "
cs-9311102,arxiv.org/abs/cs/9311102,31/10/93,Software Agents: Completing Patterns and Constructing User Interfaces,0,0,0," Software Agents: Completing Patterns and Constructing User Interfaces To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the user's information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dts/mac/sys.soft/quicktime. "
cs-9311101,arxiv.org/abs/cs/9311101,31/10/93,The Difficulties of Learning Logic Programs with Cut,0,0,0," The Difficulties of Learning Logic Programs with Cut As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages. "
cs-9312101,arxiv.org/abs/cs/9312101,30/11/93,Decidable Reasoning in Terminological Knowledge Representation Systems,0,0,0," Decidable Reasoning in Terminological Knowledge Representation Systems Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted. "
cs-9401102,arxiv.org/abs/cs/9401102,31/12/93,Mini-indexes for literate programs,0,0,0," Mini-indexes for literate programs This paper describes how to implement a documentation technique that helps readers to understand large programs or collections of programs, by providing local indexes to all identifiers that are visible on every two-page spread. A detailed example is given for a program that finds all Hamiltonian circuits in an undirected graph. "
cs-9401101,arxiv.org/abs/cs/9401101,31/12/93,Teleo-Reactive Programs for Agent Control,0,0,0," Teleo-Reactive Programs for Agent Control A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots. "
cs-9402103,arxiv.org/abs/cs/9402103,31/1/94,Bias-Driven Revision of Logical Domain Theories,0,0,0," Bias-Driven Revision of Logical Domain Theories The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``flow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories. "
cs-9402101,arxiv.org/abs/cs/9402101,31/1/94,Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models,0,0,0," Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator SPA based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms. "
cs-9402102,arxiv.org/abs/cs/9402102,31/1/94,Substructure Discovery Using Minimum Description Length and Background Knowledge,0,0,0," Substructure Discovery Using Minimum Description Length and Background Knowledge The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUE's ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value. "
cs-9403101,arxiv.org/abs/cs/9403101,28/2/94,Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction,0,0,0," Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees. "
cmp-lg-9404001,arxiv.org/abs/cmp-lg/9404001,3/4/94,An Alternative Conception of Tree-Adjoining Derivation,0,0,0," An Alternative Conception of Tree-Adjoining Derivation The precise formulation of derivation for tree-adjoining grammars has important ramifications for a wide variety of uses of the formalism, from syntactic analysis to semantic interpretation and statistical language modeling. We argue that the definition of tree-adjoining derivation must be reformulated in order to manifest the proper linguistic dependencies in derivations. The particular proposal is both precisely characterizable through a definition of TAG derivations as equivalence classes of ordered derivation trees, and computationally operational, by virtue of a compilation to linear indexed grammars together with an efficient algorithm for recognition and parsing according to the compiled grammar. "
cmp-lg-9404002,arxiv.org/abs/cmp-lg/9404002,3/4/94,Lessons from a Restricted Turing Test,0,0,0," Lessons from a Restricted Turing Test We report on the recent Loebner prize competition inspired by Turing's test of intelligent behavior. The presentation covers the structure of the competition and the outcome of its first instantiation in an actual event, and an analysis of the purpose, design, and appropriateness of such a competition. We argue that the competition has no clear purpose, that its design prevents any useful outcome, and that such a competition is inappropriate given the current level of technology. We then speculate as to suitable alternatives to the Loebner prize. "
cmp-lg-9404004,arxiv.org/abs/cmp-lg/9404004,6/4/94,An Empirically Motivated Reinterpretation of Dependency Grammar,0,0,0," An Empirically Motivated Reinterpretation of Dependency Grammar Dependency grammar is usually interpreted as equivalent to a strict form of X--bar theory that forbids the stacking of nodes of the same bar level (e.g., N' immediately dominating N' with the same head). But adequate accounts of _one_--anaphora and of the semantics of multiple modifiers require such stacking and accordingly argue against dependency grammar. Dependency grammar can be salvaged by reinterpreting its claims about phrase structure, so that modifiers map onto binary--branching X--bar trees rather than ``flat'' ones. "
cmp-lg-9404005,arxiv.org/abs/cmp-lg/9404005,11/4/94,Memoization in Constraint Logic Programming,0,0,0," Memoization in Constraint Logic Programming This paper shows how to apply memoization (caching of subgoals and associated answer substitutions) in a constraint logic programming setting. The research is is motivated by the desire to apply constraint logic programming CLP to problems in natural language processing that involve (constraint) interleaving or coroutining, such as GB and HPSG parsing. "
cmp-lg-9404006,arxiv.org/abs/cmp-lg/9404006,15/4/94,SPANISH 1992 (S92): corpus-based analysis of present-day Spanish for medical purposes,0,0,0," SPANISH 1992 (S92): corpus-based analysis of present-day Spanish for medical purposes S92 research was begun in 1987 to analyze word frequencies in present-day Spanish for making speech pathology evaluation tools. 500 2,000-word samples of children, adolescents and adults' language were input between 1988-1991, calculations done in 1992; statistical and Lewandowski analyses were carried out in 1993. "
cmp-lg-9404007,arxiv.org/abs/cmp-lg/9404007,19/4/94,Constraint-Based Categorial Grammar,0,0,0," Constraint-Based Categorial Grammar We propose a generalization of Categorial Grammar in which lexical categories are defined by means of recursive constraints. In particular, the introduction of relational constraints allows one to capture the effects of (recursive) lexical rules in a computationally attractive manner. We illustrate the linguistic merits of the new approach by showing how it accounts for the syntax of Dutch cross-serial dependencies and the position and scope of adjuncts in such constructions. Delayed evaluation is used to process grammars containing recursive constraints. "
cmp-lg-9404008,arxiv.org/abs/cmp-lg/9404008,26/4/94,Principles and Implementation of Deductive Parsing,0,0,0," Principles and Implementation of Deductive Parsing We present a system for generating parsers based directly on the metaphor of parsing as deduction. Parsing algorithms can be represented directly as deduction systems, and a single deduction engine can interpret such deduction systems so as to implement the corresponding parser. The method generalizes easily to parsers for augmented phrase structure formalisms, such as definite-clause grammars and other logic grammar formalisms, and has been used for rapid prototyping of parsing algorithms for a variety of formalisms including variants of tree-adjoining grammars, categorial grammars, and lexicalized context-free grammars. "
cmp-lg-9404011,arxiv.org/abs/cmp-lg/9404011,1/5/94,Adjuncts and the Processing of Lexical Rules,0,0,0," Adjuncts and the Processing of Lexical Rules The standard HPSG analysis of Germanic verb clusters can not explain the observed narrow-scope readings of adjuncts in such verb clusters. We present an extension of the HPSG analysis that accounts for the systematic ambiguity of the scope of adjuncts in verb cluster constructions, by treating adjuncts as members of the subcat list. The extension uses powerful recursive lexical rules, implemented as complex constraints. We show how `delayed evaluation' techniques from constraint-logic programming can be used to process such lexical rules. "
cmp-lg-9405001,arxiv.org/abs/cmp-lg/9405001,2/5/94,Similarity-Based Estimation of Word Cooccurrence Probabilities,0,0,0," Similarity-Based Estimation of Word Cooccurrence Probabilities In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations ``eat a peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ``most similar'' words. We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error. "
cmp-lg-9405003,arxiv.org/abs/cmp-lg/9405003,2/5/94,Some Bibliographical References on Intonation and Intonational Meaning,0,0,0," Some Bibliographical References on Intonation and Intonational Meaning A by-no-means-complete collection of references for those interested in intonational meaning, with other miscellaneous references on intonation included. Additional references are welcome, and should be sent to julia@research.att.com. "
cmp-lg-9405002,arxiv.org/abs/cmp-lg/9405002,2/5/94,Temporal Relations: Reference or Discourse Coherence?,0,0,0," Temporal Relations: Reference or Discourse Coherence? The temporal relations that hold between events described by successive utterances are often left implicit or underspecified. We address the role of two phenomena with respect to the recovery of these relations: (1) the referential properties of tense, and (2) the role of temporal constraints imposed by coherence relations. We account for several facets of the identification of temporal relations through an integration of these. "
cmp-lg-9405010,arxiv.org/abs/cmp-lg/9405010,3/5/94,Common Topics and Coherent Situations: Interpreting Ellipsis in the Context of Discourse Inference,0,0,0," Common Topics and Coherent Situations: Interpreting Ellipsis in the Context of Discourse Inference It is claimed that a variety of facts concerning ellipsis, event reference, and interclausal coherence can be explained by two features of the linguistic form in question: (1) whether the form leaves behind an empty constituent in the syntax, and (2) whether the form is anaphoric in the semantics. It is proposed that these features interact with one of two types of discourse inference, namely {\it Common Topic} inference and {\it Coherent Situation} inference. The differing ways in which these types of inference utilize syntactic and semantic representations predicts phenomena for which it is otherwise difficult to account. "
cmp-lg-9405006,arxiv.org/abs/cmp-lg/9405006,3/5/94,Efficiency  Robustness  and Accuracy in Picky Chart Parsing,0,0,0," Efficiency, Robustness, and Accuracy in Picky Chart Parsing This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called {\em probabilistic prediction} to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser. "
cmp-lg-9405005,arxiv.org/abs/cmp-lg/9405005,3/5/94,Pearl: A Probabilistic Chart Parser,0,0,0," Pearl: A Probabilistic Chart Parser This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the best parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar. "
cmp-lg-9405004,arxiv.org/abs/cmp-lg/9405004,3/5/94,Syntactic-Head-Driven Generation,0,0,0," Syntactic-Head-Driven Generation The previously proposed semantic-head-driven generation methods run into problems if none of the daughter constituents in the syntacto-semantic rule schemata of a grammar fits the definition of a semantic head given in Shieber et al. 1990. This is the case for the semantic analysis rules of certain constraint-based semantic representations, e.g. Underspecified Discourse Representation Structures (UDRSs) (Frank/Reyle 1992). Since head-driven generation in general has its merits, we simply return to a syntactic definition of `head' and demonstrate the feasibility of syntactic-head-driven generation. In addition to its generality, a syntactic-head-driven algorithm provides a basis for a logically well-defined treatment of the movement of (syntactic) heads, for which only ad-hoc solutions existed, so far. "
cmp-lg-9405007,arxiv.org/abs/cmp-lg/9405007,3/5/94,Towards History-based Grammars: Using Richer Models for Probabilistic Parsing,0,0,0," Towards History-based Grammars: Using Richer Models for Probabilistic Parsing We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error. "
cmp-lg-9405009,arxiv.org/abs/cmp-lg/9405009,4/5/94,Natural Language Parsing as Statistical Pattern Recognition,0,0,0," Natural Language Parsing as Statistical Pattern Recognition Traditional natural language parsers are based on rewrite rule systems developed in an arduous, time-consuming manner by grammarians. A majority of the grammarian's efforts are devoted to the disambiguation process, first hypothesizing rules which dictate constituent categories and relationships among words in ambiguous sentences, and then seeking exceptions and corrections to these rules. In this work, I propose an automatic method for acquiring a statistical parser from a set of parsed sentences which takes advantage of some initial linguistic input, but avoids the pitfalls of the iterative and seemingly endless grammar development process. Based on distributionally-derived and linguistically-based features of language, this parser acquires a set of statistical decision trees which assign a probability distribution on the space of parse trees given the input sentence. These decision trees take advantage of significant amount of contextual information, potentially including all of the lexical information in the sentence, to produce highly accurate statistical models of the disambiguation process. By basing the disambiguation criteria selection on entropy reduction rather than human intuition, this parser development method is able to consider more sentences than a human grammarian can when making individual disambiguation rules. In experiments between a parser, acquired using this statistical framework, and a grammarian's rule-based parser, developed over a ten-year period, both using the same training material and test sentences, the decision tree parser significantly outperformed the grammar-based parser on the accuracy measure which the grammarian was trying to maximize, achieving an accuracy of 78% compared to the grammar-based parser's 69%. "
cmp-lg-9405008,arxiv.org/abs/cmp-lg/9405008,5/5/94,A Stochastic Finite-State Word-Segmentation Algorithm for Chinese,0,0,0," A Stochastic Finite-State Word-Segmentation Algorithm for Chinese We present a stochastic finite-state model for segmenting Chinese text into dictionary entries and productively derived words, and providing pronunciations for these words; the method incorporates a class-based model in its treatment of personal names. We also evaluate the system's performance, taking into account the fact that people often do not agree on a single segmentation. "
cmp-lg-9405011,arxiv.org/abs/cmp-lg/9405011,6/5/94,A Plan-Based Model for Response Generation in Collaborative Task-Oriented Dialogues,0,0,0," A Plan-Based Model for Response Generation in Collaborative Task-Oriented Dialogues This paper presents a plan-based architecture for response generation in collaborative consultation dialogues, with emphasis on cases in which the system (consultant) and user (executing agent) disagree. Our work contributes to an overall system for collaborative problem-solving by providing a plan-based framework that captures the {\em Propose-Evaluate-Modify} cycle of collaboration, and by allowing the system to initiate subdialogues to negotiate proposed additions to the shared plan and to provide support for its claims. In addition, our system handles in a unified manner the negotiation of proposed domain actions, proposed problem-solving actions, and beliefs proposed by discourse actions. Furthermore, it captures cooperative responses within the collaborative framework and accounts for why questions are sometimes never answered. "
cmp-lg-9405012,arxiv.org/abs/cmp-lg/9405012,6/5/94,Integration Of Visual Inter-word Constraints And Linguistic Knowledge In Degraded Text Recognition,0,0,0," Integration Of Visual Inter-word Constraints And Linguistic Knowledge In Degraded Text Recognition Degraded text recognition is a difficult task. Given a noisy text image, a word recognizer can be applied to generate several candidates for each word image. High-level knowledge sources can then be used to select a decision from the candidate set for each word image. In this paper, we propose that visual inter-word constraints can be used to facilitate candidate selection. Visual inter-word constraints provide a way to link word images inside the text page, and to interpret them systematically. "
cmp-lg-9405014,arxiv.org/abs/cmp-lg/9405014,9/5/94,Classifying Cue Phrases in Text and Speech Using Machine Learning,0,0,0," Classifying Cue Phrases in Text and Speech Using Machine Learning Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification rules from sets of pre-classified cue phrases and their features. Machine learning is shown to be an effective technique for not only automating the generation of classification rules, but also for improving upon previous results. "
cmp-lg-9405013,arxiv.org/abs/cmp-lg/9405013,9/5/94,Collaboration on reference to objects that are not mutually known,0,0,0," Collaboration on reference to objects that are not mutually known In conversation, a person sometimes has to refer to an object that is not previously known to the other participant. We present a plan-based model of how agents collaborate on reference of this sort. In making a reference, an agent uses the most salient attributes of the referent. In understanding a reference, an agent determines his confidence in its adequacy as a means of identifying the referent. To collaborate, the agents use judgment, suggestion, and elaboration moves to refashion an inadequate referring expression. "
cmp-lg-9405015,arxiv.org/abs/cmp-lg/9405015,9/5/94,Intention-based Segmentation: Human Reliability and Correlation with Linguistic Cues,0,0,0," Intention-based Segmentation: Human Reliability and Correlation with Linguistic Cues Certain spans of utterances in a discourse, referred to here as segments, are widely assumed to form coherent units. Further, the segmental structure of discourse has been claimed to constrain and be constrained by many phenomena. However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them. We present quantitative results of a two part study using a corpus of spontaneous, narrative monologues. The first part evaluates the statistical reliability of human segmentation of our corpus, where speaker intention is the segmentation criterion. We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics. "
cmp-lg-9405017,arxiv.org/abs/cmp-lg/9405017,10/5/94,Best-first Model Merging for Hidden Markov Model Induction,0,0,0," Best-first Model Merging for Hidden Markov Model Induction This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general `model merging' strategy (Omohundro 1992). The process begins with a maximum likelihood HMM that directly encodes the training data. Successively more general models are produced by merging HMM states. A Bayesian posterior probability criterion is used to determine which states to merge and when to stop generalizing. The procedure may be considered a heuristic search for the HMM structure with the highest posterior probability. We discuss a variety of possible priors for HMMs, as well as a number of approximations which improve the computational efficiency of the algorithm. We studied three applications to evaluate the procedure. The first compares the merging algorithm with the standard Baum-Welch approach in inducing simple finite-state languages from small, positive-only training samples. We found that the merging procedure is more robust and accurate, particularly with a small amount of training data. The second application uses labelled speech data from the TIMIT database to build compact, multiple-pronunciation word models that can be used in speech recognition. Finally, we describe how the algorithm was incorporated in an operational speech understanding system, where it is combined with neural network acoustic likelihood estimators to improve performance over single-pronunciation word models. "
cmp-lg-9405016,arxiv.org/abs/cmp-lg/9405016,10/5/94,Precise n-gram Probabilities from Stochastic Context-free Grammars,0,0,0," Precise n-gram Probabilities from Stochastic Context-free Grammars We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. We discuss efficient implementation of the algorithm and report our practical experience with it. "
cmp-lg-9405018,arxiv.org/abs/cmp-lg/9405018,16/5/94,Memory-Based Lexical Acquisition and Processing,0,0,0," Memory-Based Lexical Acquisition and Processing Current approaches to computational lexicology in language technology are knowledge-based (competence-oriented) and try to abstract away from specific formalisms, domains, and applications. This results in severe complexity, acquisition and reusability bottlenecks. As an alternative, we propose a particular performance-oriented approach to Natural Language Processing based on automatic memory-based learning of linguistic (lexical) tasks. The consequences of the approach for computational lexicology are discussed, and the application of the approach on a number of lexical acquisition and disambiguation tasks in phonology, morphology and syntax is described. "
cmp-lg-9405019,arxiv.org/abs/cmp-lg/9405019,23/5/94,Determination of referential property and number of nouns in Japanese sentences for machine translation into English,0,0,0," Determination of referential property and number of nouns in Japanese sentences for machine translation into English When translating Japanese nouns into English, we face the problem of articles and numbers which the Japanese language does not have, but which are necessary for the English composition. To solve this difficult problem we classified the referential property and the number of nouns into three types respectively. This paper shows that the referential property and the number of nouns in a sentence can be estimated fairly reliably by the words in the sentence. Many rules for the estimation were written in forms similar to rewriting rules in expert systems. We obtained the correct recognition scores of 85.5\% and 89.0\% in the estimation of the referential property and the number respectively for the sentences which were used for the construction of our rules. We tested these rules for some other texts, and obtained the scores of 68.9\% and 85.6\% respectively. "
cmp-lg-9405020,arxiv.org/abs/cmp-lg/9405020,24/5/94,Capturing CFLs with Tree Adjoining Grammars,0,0,0, Capturing CFLs with Tree Adjoining Grammars We define a decidable class of TAGs that is strongly equivalent to CFGs and is cubic-time parsable. This class serves to lexicalize CFGs in the same manner as the LCFGs of Schabes and Waters but with considerably less restriction on the form of the grammars. The class provides a normal form for TAGs that generate local sets in much the same way that regular grammars provide a normal form for CFGs that generate regular sets. 
cmp-lg-9405021,arxiv.org/abs/cmp-lg/9405021,24/5/94,Generating Precondition Expressions in Instructional Text,0,0,0," Generating Precondition Expressions in Instructional Text This study employs a knowledge intensive corpus analysis to identify the elements of the communicative context which can be used to determine the appropriate lexical and grammatical form of instructional texts. \ig, an instructional text generation system based on this analysis, is presented, particularly with reference to its expression of precondition relations. "
cmp-lg-9405023,arxiv.org/abs/cmp-lg/9405023,25/5/94,An Integrated Heuristic Scheme for Partial Parse Evaluation,0,0,0," An Integrated Heuristic Scheme for Partial Parse Evaluation GLR* is a recently developed robust version of the Generalized LR Parser, that can parse almost ANY input sentence by ignoring unrecognizable parts of the sentence. On a given input sentence, the parser returns a collection of parses that correspond to maximal, or close to maximal, parsable subsets of the original input. This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed ``best'' from such a collection. We describe the heuristic measures used and their combination scheme. Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported. "
cmp-lg-9405022,arxiv.org/abs/cmp-lg/9405022,25/5/94,Grammar Specialization through Entropy Thresholds,0,0,0," Grammar Specialization through Entropy Thresholds Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees. This allows very much faster parsing and gives a lower error rate, at the price of a small loss in coverage. Previously, it has been necessary to specify the tree-cutting criteria (or operationality criteria) manually; here they are derived automatically from the training set and the desired coverage of the specialized grammar. This is done by assigning an entropy value to each node in the parse trees and cutting in the nodes with sufficiently high entropy values. "
cmp-lg-9405024,arxiv.org/abs/cmp-lg/9405024,26/5/94,Abductive Equivalential Translation and its application to Natural Language Database Interfacing,0,0,0," Abductive Equivalential Translation and its application to Natural Language Database Interfacing The thesis describes a logical formalization of natural-language database interfacing. We assume the existence of a ``natural language engine'' capable of mediating between surface linguistic string and their representations as ``literal'' logical forms: the focus of interest will be the question of relating ``literal'' logical forms to representations in terms of primitives meaningful to the underlying database engine. We begin by describing the nature of the problem, and show how a variety of interface functionalities can be considered as instances of a type of formal inference task which we call ``Abductive Equivalential Translation'' AET; functionalities which can be reduced to this form include answering questions, responding to commands, reasoning about the completeness of answers, answering meta-questions of type ``Do you know...'', and generating assertions and questions. In each case, a ``linguistic domain theory'' LDT and an input formula are given, and the goal is to construct a formula with certain properties which is equivalent to , given and a set of permitted assumptions. If the LDT is of a certain specified type, whose formulas are either conditional equivalences or Horn-clauses, we show that the AET problem can be reduced to a goal-directed inference method. We present an abstract description of this method, and sketch its realization in Prolog. The relationship between AET and several problems previously discussed in the literature is discussed. In particular, we show how AET can provide a simple and elegant solution to the so-called ``Doctor on Board'' problem, and in effect allows a ``relativization'' of the Closed World Assumption. The ideas in the thesis have all been implemented concretely within the SRI CLARE project, using a real projects and payments database. The LDT for the example database is described in detail, and examples of the types of functionality that can be achieved within the example domain are presented. "
cmp-lg-9405026,arxiv.org/abs/cmp-lg/9405026,26/5/94,An Extended Theory of Head-Driven Parsing,0,0,0," An Extended Theory of Head-Driven Parsing We show that more head-driven parsing algorithms can be formulated than those occurring in the existing literature. These algorithms are inspired by a family of left-to-right parsing algorithms from a recent publication. We further introduce a more advanced notion of ``head-driven parsing'' which allows more detailed specification of the processing order of non-head elements in the right-hand side. We develop a parsing algorithm for this strategy, based on LR parsing techniques. "
cmp-lg-9405025,arxiv.org/abs/cmp-lg/9405025,26/5/94,An Optimal Tabular Parsing Algorithm,0,0,0," An Optimal Tabular Parsing Algorithm In this paper we relate a number of parsing algorithms which have been developed in very different areas of parsing theory, and which include deterministic algorithms, tabular algorithms, and a parallel algorithm. We show that these algorithms are based on the same underlying ideas. By relating existing ideas, we hope to provide an opportunity to improve some algorithms based on features of others. A second purpose of this paper is to answer a question which has come up in the area of tabular parsing, namely how to obtain a parsing algorithm with the property that the table will contain as little entries as possible, but without the possibility that two entries represent the same subderivation. "
cmp-lg-9405027,arxiv.org/abs/cmp-lg/9405027,27/5/94,Acquiring Receptive Morphology: A Connectionist Model,0,0,0," Acquiring Receptive Morphology: A Connectionist Model This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology. The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections. Simulations using artificial language stimuli demonstrate the capacity of the model to learn suffixation, prefixation, infixation, circumfixation, mutation, template, and deletion rules. Separate network modules responsible for syllables enable to the network to learn simple reduplication rules as well. The model also embodies constraints against association-line crossing. "
cmp-lg-9404009,arxiv.org/abs/cmp-lg/9404009,27/5/94,A Deductive Account of Quantification in LFG,0,0,0," A Deductive Account of Quantification in LFG The relationship between Lexical-Functional Grammar LFG functional structures (f-structures) for sentences and their semantic interpretations can be expressed directly in a fragment of linear logic in a way that explains correctly the constrained interactions between quantifier scope ambiguity and bound anaphora. The use of a deductive framework to account for the compositional properties of quantifying expressions in natural language obviates the need for additional mechanisms, such as Cooper storage, to represent the different scopes that a quantifier might take. Instead, the semantic contribution of a quantifier is recorded as an ordinary logical formula, one whose use in a proof will establish the scope of the quantifier. The properties of linear logic ensure that each quantifier is scoped exactly once. Our analysis of quantifier scope can be seen as a recasting of Pereira's analysis (Pereira, 1991), which was expressed in higher-order intuitionistic logic. But our use of LFG and linear logic provides a much more direct and computationally more flexible interpretation mechanism for at least the same range of phenomena. We have developed a preliminary Prolog implementation of the linear deductions described in this work. "
cmp-lg-9405028,arxiv.org/abs/cmp-lg/9405028,28/5/94,Semantics of Complex Sentences in Japanese,0,0,0," Semantics of Complex Sentences in Japanese The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively. However if there can be relations between every pair of semantic roles, the amount of computation to identify the relations that hold in the given sentence is extremely large. In this paper, for semantics of Japanese complex sentence, we introduce new pragmatic roles called `observer' and `motivated' respectively to bridge semantic roles of subordinate and those of main clauses. By these new roles constraints on the relations among semantic/pragmatic roles are known to be almost local within subordinate or main clause. In other words, as for the semantics of the whole complex sentence, the only role we should deal with is a motivated. "
cmp-lg-9405031,arxiv.org/abs/cmp-lg/9405031,30/5/94,An Attributive Logic of Set Descriptions and Set Operations,0,0,0," An Attributive Logic of Set Descriptions and Set Operations This paper provides a model theoretic semantics to feature terms augmented with set descriptions. We provide constraints to specify HPSG style set descriptions, fixed cardinality set descriptions, set-membership constraints, restricted universal role quantifications, set union, intersection, subset and disjointness. A sound, complete and terminating consistency checking procedure is provided to determine the consistency of any given term in the logic. It is shown that determining consistency of terms is a NP-complete problem. "
cmp-lg-9405032,arxiv.org/abs/cmp-lg/9405032,30/5/94,Modularity in a Connectionist Model of Morphology Acquisition,0,0,0," Modularity in a Connectionist Model of Morphology Acquisition This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology. The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections. In its simplest version, the network consists of separate simple recurrent subnetworks for root and inflection identification; both networks take the phone sequence as inputs. It is shown that the performance of the two separate modular networks is superior to a single network responsible for both root and inflection identification. In a more elaborate version of the model, the network learns to use separate hidden-layer modules to solve the separate tasks of root and inflection identification. "
cmp-lg-9405030,arxiv.org/abs/cmp-lg/9405030,30/5/94,Priority Union and Generalization in Discourse Grammars,0,0,0," Priority Union and Generalization in Discourse Grammars We describe an implementation in Carpenter's typed feature formalism, ALE, of a discourse grammar of the kind proposed by Scha, Polanyi, et al. We examine their method for resolving parallelism-dependent anaphora and show that there is a coherent feature-structural rendition of this type of grammar which uses the operations of priority union and generalization. We describe an augmentation of the ALE system to encompass these operations and we show that an appropriate choice of definition for priority union gives the desired multiple output for examples of VP-ellipsis which exhibit a strict/sloppy ambiguity. "
cmp-lg-9405029,arxiv.org/abs/cmp-lg/9405029,30/5/94,Structural Tags  Annealing and Automatic Word Classification,0,0,0," Structural Tags, Annealing and Automatic Word Classification This paper describes an automatic word classification system which uses a locally optimal annealing algorithm and average class mutual information. A new word-class representation, the structural tag is introduced and its advantages for use in statistical language modelling are presented. A summary of some results with the one million word LOB corpus is given; the algorithm is also shown to discover the vowel-consonant distinction and displays an ability to cluster words syntactically in a Latin corpus. Finally, a comparison is made between the current classification system and several leading alternative systems, which shows that the current system performs respectably well. "
cs-9406102,arxiv.org/abs/cs/9406102,31/5/94,Applying GSAT to Non-Clausal Formulas,0,0,0," Applying GSAT to Non-Clausal Formulas In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a particular ``score'' function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far. "
cs-9406101,arxiv.org/abs/cs/9406101,31/5/94,A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic,0,0,0," A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest. "
cmp-lg-9405035,arxiv.org/abs/cmp-lg/9405035,31/5/94,Dual-Coding Theory and Connectionist Lexical Selection,0,0,0," Dual-Coding Theory and Connectionist Lexical Selection We introduce the bilingual dual-coding theory as a model for bilingual mental representation. Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation. This lexical selection approach has two advantages. First, it is learnable. Little human effort on knowledge engineering is required. Secondly, it is psycholinguistically well-founded. "
cmp-lg-9405034,arxiv.org/abs/cmp-lg/9405034,31/5/94,Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and Its Automatic Evaluation,0,0,0," Extracting Noun Phrases from Large-Scale Texts: A Hybrid Approach and Its Automatic Evaluation To acquire noun phrases from running texts is useful for many applications, such as word grouping,terminology indexing, etc. The reported literatures adopt pure probabilistic approach, or pure rule-based noun phrases grammar to tackle this problem. In this paper, we apply a probabilistic chunker to deciding the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism. The test texts are SUSANNE Corpus and the results are evaluated by comparing the parse field of SUSANNE Corpus automatically. The results of this preliminary experiment are encouraging. "
cmp-lg-9405033,arxiv.org/abs/cmp-lg/9405033,31/5/94,Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars,0,0,0," Relating Complexity to Practical Performance in Parsing with Wide-Coverage Unification Grammars The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms, using a wide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers. "
cmp-lg-9406003,arxiv.org/abs/cmp-lg/9406003,1/6/94,A Learning Approach to Natural Language Understanding,0,0,0," A Learning Approach to Natural Language Understanding In this paper we propose a learning paradigm for the problem of understanding spoken language. The basis of the work is in a formalization of the understanding problem as a communication problem. This results in the definition of a stochastic model of the production of speech or text starting from the meaning of a sentence. The resulting understanding algorithm consists in a Viterbi maximization procedure, analogous to that commonly used for recognizing speech. The algorithm was implemented for building "
cmp-lg-9406001,arxiv.org/abs/cmp-lg/9406001,1/6/94,Intentions and Information in Discourse,0,0,0," Intentions and Information in Discourse This paper is about the flow of inference between communicative intentions, discourse structure and the domain during discourse processing. We augment a theory of discourse interpretation with a theory of distinct mental attitudes and reasoning about them, in order to provide an account of how the attitudes interact with reasoning about discourse structure. "
cmp-lg-9406002,arxiv.org/abs/cmp-lg/9406002,1/6/94,Speech Dialogue with Facial Displays: Multimodal Human-Computer Conversation,0,0,0," Speech Dialogue with Facial Displays: Multimodal Human-Computer Conversation Human face-to-face conversation is an ideal model for human-computer dialogue. One of the major features of face-to-face communication is its multiplicity of communication channels that act on multiple modalities. To realize a natural multimodal dialogue, it is necessary to study how humans perceive information and determine the information to which humans are sensitive. A face is an independent communication channel that conveys emotional and conversational signals, encoded as facial expressions. We have developed an experimental system that integrates speech dialogue and facial animation, to investigate the effect of introducing communicative facial expressions as a new modality in human-computer conversation. Our experiments have shown that facial expressions are helpful, especially upon first contact with the system. We have also discovered that featuring facial expressions at an early stage improves subsequent interaction. "
cmp-lg-9406004,arxiv.org/abs/cmp-lg/9406004,1/6/94,Towards a Principled Representation of Discourse Plans,0,0,0," Towards a Principled Representation of Discourse Plans We argue that discourse plans must capture the intended causal and decompositional relations between communicative actions. We present a planning algorithm, DPOCL, that builds plan structures that properly capture these relations, and show how these structures are used to solve the problems that plagued previous discourse planners, and allow a system to participate effectively and flexibly in an ongoing dialogue. "
cmp-lg-9406005,arxiv.org/abs/cmp-lg/9406005,1/6/94,Word-Sense Disambiguation Using Decomposable Models,0,0,0," Word-Sense Disambiguation Using Decomposable Models Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features. In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun interest . We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model. Using this approach, the joint distribution of all variables is described by only the most systematic variable interactions, thereby limiting the number of parameters to be estimated, supporting computational efficiency, and providing an understanding of the data. "
cmp-lg-9406007,arxiv.org/abs/cmp-lg/9406007,2/6/94,Aligning a Parallel English-Chinese Corpus Statistically with Lexical Criteria,0,0,0, Aligning a Parallel English-Chinese Corpus Statistically with Lexical Criteria We describe our experience with automatic alignment of sentences in parallel English-Chinese texts. Our report concerns three related topics: (1) progress on the HKUST English-Chinese Parallel Bilingual Corpus; (2) experiments addressing the applicability of Gale & Church's length-based statistical method to the task of alignment involving a non-Indo-European language; and (3) an improved statistical method that also incorporates domain-specific lexical cues. 
cmp-lg-9406006,arxiv.org/abs/cmp-lg/9406006,2/6/94,Detecting and Correcting Speech Repairs,0,0,0," Detecting and Correcting Speech Repairs Interactive spoken dialog provides many new challenges for spoken language systems. One of the most critical is the prevalence of speech repairs. This paper presents an algorithm that detects and corrects speech repairs based on finding the repair pattern. The repair pattern is built by finding word matches and word replacements, and identifying fragments and editing terms. Rather than using a set of prebuilt templates, we build the pattern on the fly. In a fair test, our method, when combined with a statistical model to filter possible repairs, was successful at detecting and correcting 80\% of the repairs, without using prosodic information or a parser. "
cmp-lg-9406009,arxiv.org/abs/cmp-lg/9406009,2/6/94,Multiset-Valued Linear Index Grammars: Imposing Dominance Constraints on Derivations,0,0,0," Multiset-Valued Linear Index Grammars: Imposing Dominance Constraints on Derivations This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links. The former models certain uses of multiset-valued feature structures in unification-based formalisms, while the latter is motivated by word order variation and by ``quasi-trees'', a generalization of trees. The two formalisms are weakly equivalent, and an important subset is at most context-sensitive and polynomially parsable. "
cmp-lg-9406008,arxiv.org/abs/cmp-lg/9406008,2/6/94,Parsing Turkish with the Lexical Functional Grammar Formalism,0,0,0," Parsing Turkish with the Lexical Functional Grammar Formalism This paper describes our work on parsing Turkish using the lexical-functional grammar formalism. This work represents the first significant effort for parsing Turkish. Our implementation is based on Tomita's parser developed at Carnegie-Mellon University Center for Machine Translation. The grammar covers a substantial subset of Turkish including simple and complex sentences, and deals with a reasonable amount of word order freeness. The complex agglutinative morphology of Turkish lexical structures is handled using a separate two-level morphological analyzer. After a discussion of key relevant issues regarding Turkish grammar, we discuss aspects of our system and present results from our implementation. Our initial results suggest that our system can parse about 82\% of the sentences directly and almost all the remaining with very minor pre-editing. "
cmp-lg-9406010,arxiv.org/abs/cmp-lg/9406010,2/6/94,Some Advances in Transformation-Based Part of Speech Tagging,0,0,0," Some Advances in Transformation-Based Part of Speech Tagging Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In [Brill92], a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that are not captured by stochastic taggers. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty. "
cmp-lg-9406011,arxiv.org/abs/cmp-lg/9406011,3/6/94,Exploring the Statistical Derivation of Transformational Rule Sequences for Part-of-Speech Tagging,0,0,0," Exploring the Statistical Derivation of Transformational Rule Sequences for Part-of-Speech Tagging Eric Brill has recently proposed a simple and powerful corpus-based language modeling approach that can be applied to various tasks including part-of-speech tagging and building phrase structure trees. The method learns a series of symbolic transformational rules, which can then be applied in sequence to a test corpus to produce predictions. The learning process only requires counting matches for a given set of rule templates, allowing the method to survey a very large space of possible contextual factors. This paper analyses Brill's approach as an interesting variation on existing decision tree methods, based on experiments involving part-of-speech tagging for both English and ancient Greek corpora. In particular, the analysis throws light on why the new mechanism seems surprisingly resistant to overtraining. A fast, incremental implementation and a mechanism for recording the dependencies that underlie the resulting rule sequence are also described. "
cmp-lg-9406012,arxiv.org/abs/cmp-lg/9406012,3/6/94,Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions,0,0,0," Self-Organizing Machine Translation: Example-Driven Induction of Transfer Functions With the advent of faster computers, the notion of doing machine translation from a huge stored database of translation examples is no longer unreasonable. This paper describes an attempt to merge the Example-Based Machine Translation EBMT approach with psycholinguistic principles. A new formalism for context- free grammars, called *marker-normal form*, is demonstrated and used to describe language data in a way compatible with psycholinguistic theories. By embedding this formalism in a standard multivariate optimization framework, a system can be built that infers correct transfer functions for a set of bilingual sentence pairs and then uses those functions to translate novel sentences. The validity of this line of reasoning has been tested in the development of a system called METLA-1. This system has been used to infer English->French and English->Urdu transfer functions from small corpora. The results of those experiments are examined, both in engineering terms as well as in more linguistic terms. In general, the results of these experiments were psycho- logically and linguistically well-grounded while still achieving a respectable level of success when compared against a similar prototype using Hidden Markov Models. "
cmp-lg-9406013,arxiv.org/abs/cmp-lg/9406013,5/6/94,Graded Unification: A Framework for Interactive Processing,0,0,0," Graded Unification: A Framework for Interactive Processing An extension to classical unification, called {\em graded unification} is presented. It is capable of combining contradictory information. An interactive processing paradigm and parser based on this new operator are also presented. "
cmp-lg-9406014,arxiv.org/abs/cmp-lg/9406014,7/6/94,A Hybrid Reasoning Model for Indirect Answers,0,0,0," A Hybrid Reasoning Model for Indirect Answers This paper presents our implemented computational model for interpreting and generating indirect answers to Yes-No questions. Its main features are 1) a discourse-plan-based approach to implicature, 2) a reversible architecture for generation and interpretation, 3) a hybrid reasoning model that employs both plan inference and logical inference, and 4) use of stimulus conditions to model a speaker's motivation for providing appropriate, unrequested information. The model handles a wider range of types of indirect answers than previous computational models and has several significant advantages. "
cmp-lg-9406017,arxiv.org/abs/cmp-lg/9406017,7/6/94,An Automatic Method of Finding Topic Boundaries,0,0,0," An Automatic Method of Finding Topic Boundaries This article outlines a new method of locating discourse boundaries based on lexical cohesion and a graphical technique called dotplotting. The application of dotplotting to discourse segmentation can be performed either manually, by examining a graph, or automatically, using an optimization algorithm. The results of two experiments involving automatically locating boundaries between a series of concatenated documents are presented. Areas of application and future directions for this work are also outlined. "
cmp-lg-9406016,arxiv.org/abs/cmp-lg/9406016,7/6/94,Corpus-Driven Knowledge Acquisition for Discourse Analysis,0,0,0," Corpus-Driven Knowledge Acquisition for Discourse Analysis The availability of large on-line text corpora provides a natural and promising bridge between the worlds of natural language processing NLP and machine learning ML. In recent years, the NLP community has been aggressively investigating statistical techniques to drive part-of-speech taggers, but application-specific text corpora can be used to drive knowledge acquisition at much higher levels as well. In this paper we will show how ML techniques can be used to support knowledge acquisition for information extraction systems. It is often very difficult to specify an explicit domain model for many information extraction applications, and it is always labor intensive to implement hand-coded heuristics for each new domain. We have discovered that it is nevertheless possible to use ML algorithms in order to capture knowledge that is only implicitly present in a representative text corpus. Our work addresses issues traditionally associated with discourse analysis and intersentential inference generation, and demonstrates the utility of ML algorithms at this higher level of language analysis. The benefits of our work address the portability and scalability of information extraction IE technologies. When hand-coded heuristics are used to manage discourse analysis in an information extraction system, months of programming effort are easily needed to port a successful IE system to a new domain. We will show how ML algorithms can reduce this "
cmp-lg-9406015,arxiv.org/abs/cmp-lg/9406015,7/6/94,Statistical Augmentation of a Chinese Machine-Readable Dictionary,0,0,0," Statistical Augmentation of a Chinese Machine-Readable Dictionary We describe a method of using statistically-collected Chinese character groups from a corpus to augment a Chinese dictionary. The method is particularly useful for extracting domain-specific and regional words not readily available in machine-readable dictionaries. Output was evaluated both using human evaluators and against a previously available dictionary. We also evaluated performance improvement in automatic Chinese tokenization. Results show that our method outputs legitimate words, acronymic constructions, idioms, names and titles, as well as technical compounds, many of which were lacking from the original dictionary. "
cmp-lg-9406020,arxiv.org/abs/cmp-lg/9406020,10/6/94,DPOCL: A Principled Approach to Discourse Planning,0,0,0," DPOCL: A Principled Approach to Discourse Planning Research in discourse processing has identified two representational requirements for discourse planning systems. First, discourse plans must adequately represent the intentional structure of the utterances they produce in order to enable a computational discourse agent to respond effectively to communicative failures \cite{MooreParisCL}. Second, discourse plans must represent the informational structure of utterances. In addition to these representational requirements, we argue that discourse planners should be formally characterizable in terms of soundness and completeness. "
cmp-lg-9406022,arxiv.org/abs/cmp-lg/9406022,13/6/94,An implemented model of punning riddles,0,0,0," An implemented model of punning riddles In this paper, we discuss a model of simple question-answer punning, implemented in a program, JAPE, which generates riddles from humour-independent lexical entries. The model uses two main types of structure: schemata, which determine the relationships between key words in a joke, and templates, which produce the surface form of the joke. JAPE succeeds in generating pieces of text that are recognizably jokes, but some of them are not very good jokes. We mention some potential improvements and extensions, including post-production heuristics for ordering the jokes according to quality. "
cmp-lg-9406021,arxiv.org/abs/cmp-lg/9406021,13/6/94,A symbolic description of punning riddles and its computer implementation,0,0,0," A symbolic description of punning riddles and its computer implementation Riddles based on simple puns can be classified according to the patterns of word, syllable or phrase similarity they depend upon. We have devised a formal model of the semantic and syntactic regularities underlying some of the simpler types of punning riddle. We have also implemented this preliminary theory in a computer program which can generate riddles from a lexicon containing general data about words and phrases; that is, the lexicon content is not customised to produce jokes. Informal evaluation of the program's results by a set of human judges suggest that the riddles produced by this program are of comparable quality to those in general circulation among school children. "
cmp-lg-9406023,arxiv.org/abs/cmp-lg/9406023,14/6/94,A Spanish Tagset for the CRATER Project,0,0,0," A Spanish Tagset for the CRATER Project This working paper describes the Spanish tagset to be used in the context of CRATER, a CEC funded project aiming at the creation of a multilingual (English, French, Spanish) aligned corpus using the International Telecommunications Union corpus. In this respect, each version of the corpus will be (or is currently) tagged. Xerox PARC tagger will be adapted to Spanish in order to perform the tagging of the Spanish version. This tagset has been devised as the ideal one for Spanish, and has been posted to several lists in order to get feedback to it. "
cmp-lg-9406018,arxiv.org/abs/cmp-lg/9406018,15/6/94,TDL--- A Type Description Language for Constraint-Based Grammars,0,0,0," TDL--- A Type Description Language for Constraint-Based Grammars This paper presents \tdl, a typed feature-based representation language and inference system. Type definitions in \tdl\ consist of type and feature constraints over the boolean connectives. \tdl\ supports open- and closed-world reasoning over types and allows for partitions and incompatible types. Working with partially as well as with fully expanded types is possible. Efficient reasoning in \tdl\ is accomplished through specialized modules. "
cmp-lg-9406025,arxiv.org/abs/cmp-lg/9406025,16/6/94,Emergent Parsing and Generation with Generalized Chart,0,0,0," Emergent Parsing and Generation with Generalized Chart A new, flexible inference method for Horn logic program is proposed, which is a drastic generalization of chart parsing, partial instantiations of clauses in a program roughly corresponding to arcs in a chart. Chart-like parsing and semantic-head-driven generation emerge from this method. With a parsimonious instantiation scheme for ambiguity packing, the parsing complexity reduces to that of standard chart-based algorithms. "
cmp-lg-9406024,arxiv.org/abs/cmp-lg/9406024,16/6/94,Learning Fault-tolerant Speech Parsing with SCREEN,0,0,0," Learning Fault-tolerant Speech Parsing with SCREEN This paper describes a new approach and a system SCREEN for fault-tolerant speech parsing. SCREEEN stands for Symbolic Connectionist Robust EnterprisE for Natural language. Speech parsing describes the syntactic and semantic analysis of spontaneous spoken language. The general approach is based on incremental immediate flat analysis, learning of syntactic and semantic speech parsing, parallel integration of current hypotheses, and the consideration of various forms of speech related errors. The goal for this approach is to explore the parallel interactions between various knowledge sources for learning incremental fault-tolerant speech parsing. This approach is examined in a system SCREEN using various hybrid connectionist techniques. Hybrid connectionist techniques are examined because of their promising properties of inherent fault tolerance, learning, gradedness and parallel constraint integration. The input for SCREEN is hypotheses about recognized words of a spoken utterance potentially analyzed by a speech system, the output is hypotheses about the flat syntactic and semantic analysis of the utterance. In this paper we focus on the general approach, the overall architecture, and examples for learning flat syntactic speech parsing. Different from most other speech language architectures SCREEN emphasizes an interactive rather than an autonomous position, learning rather than encoding, flat analysis rather than in-depth analysis, and fault-tolerant processing of phonetic, syntactic and semantic knowledge. "
cmp-lg-9406026,arxiv.org/abs/cmp-lg/9406026,16/6/94,The Very Idea of Dynamic Semantics,0,0,0," The Very Idea of Dynamic Semantics Natural languages are programming languages for minds. Can we or should we take this slogan seriously? If so, how? Can answers be found by looking at the various dynamic treatments of natural language developed over the last decade or so, mostly in response to problems associated with donkey anaphora? In Dynamic Logic of Programs, the meaning of a program is a binary relation on the set of states of some abstract machine. This relation is meant to model aspects of the effects of the execution of the program, in particular its input-output behavior. What, if anything, are the dynamic aspects of various proposed dynamic semantics for natural languages supposed to model? Is there anything dynamic to be modeled? If not, what is all the full about? We shall try to answer some, at least, of these questions and provide materials for answers to others. "
cmp-lg-9406019,arxiv.org/abs/cmp-lg/9406019,17/6/94,A Complete and Recursive Feature Theory,0,0,0," A Complete and Recursive Feature Theory Various feature descriptions are being employed in logic programming languages and constrained-based grammar formalisms. The common notational primitive of these descriptions are functional attributes called features. The descriptions considered in this paper are the possibly quantified first-order formulae obtained from a signature of binary and unary predicates called features and sorts, respectively. We establish a first-order theory FT by means of three axiom schemes, show its completeness, and construct three elementarily equivalent models. One of the models consists of so-called feature graphs, a data structure common in computational linguistics. The other two models consist of so-called feature trees, a record-like data structure generalizing the trees corresponding to first-order terms. Our completeness proof exhibits a terminating simplification system deciding validity and satisfiability of possibly quantified feature descriptions. "
cmp-lg-9406027,arxiv.org/abs/cmp-lg/9406027,17/6/94,Analyzing and Improving Statistical Language Models for Speech Recognition,0,0,0," Analyzing and Improving Statistical Language Models for Speech Recognition In many current speech recognizers, a statistical language model is used to indicate how likely it is that a certain word will be spoken next, given the words recognized so far. How can statistical language models be improved so that more complex speech recognition tasks can be tackled? Since the knowledge of the weaknesses of any theory often makes improving the theory easier, the central idea of this thesis is to analyze the weaknesses of existing statistical language models in order to subsequently improve them. To that end, we formally define a weakness of a statistical language model in terms of the logarithm of the total probability, LTP, a term closely related to the standard perplexity measure used to evaluate statistical language models. We apply our definition of a weakness to a frequently used statistical language model, called a bi-pos model. This results, for example, in a new modeling of unknown words which improves the performance of the model by 14% to 21%. Moreover, one of the identified weaknesses has prompted the development of our generalized N-pos language model, which is also outlined in this thesis. It can incorporate linguistic knowledge even if it extends over many words and this is not feasible in a traditional N-pos model. This leads to a discussion of whatknowledge should be added to statistical language models in general and we give criteria for selecting potentially useful knowledge. These results show the usefulness of both our definition of a weakness and of performing an analysis of weaknesses of statistical language models in general. "
cmp-lg-9406029,arxiv.org/abs/cmp-lg/9406029,20/6/94,A Computational Model of Syntactic Processing: Ambiguity Resolution from Interpretation,0,0,0," A Computational Model of Syntactic Processing: Ambiguity Resolution from Interpretation Syntactic ambiguity abounds in natural language, yet humans have no difficulty coping with it. In fact, the process of ambiguity resolution is almost always unconscious. But it is not infallible, however, as example 1 demonstrates. 1. The horse raced past the barn fell. This sentence is perfectly grammatical, as is evident when it appears in the following context: 2. Two horses were being shown off to a prospective buyer. One was raced past a meadow. and the other was raced past a barn. ... Grammatical yet unprocessable sentences such as 1 are called `garden-path sentences.' Their existence provides an opportunity to investigate the human sentence processing mechanism by studying how and when it fails. The aim of this thesis is to construct a computational model of language understanding which can predict processing difficulty. The data to be modeled are known examples of garden path and non-garden path sentences, and other results from psycholinguistics. It is widely believed that there are two distinct loci of computation in sentence processing: syntactic parsing and semantic interpretation. One longstanding controversy is which of these two modules bears responsibility for the immediate resolution of ambiguity. My claim is that it is the latter, and that the syntactic processing module is a very simple device which blindly and faithfully constructs all possible analyses for the sentence up to the current point of processing. The interpretive module serves as a filter, occasionally discarding certain of these analyses which it deems less appropriate for the ongoing discourse than their competitors. This document is divided into three parts. The first is introductory, and reviews a selection of proposals from the sentence processing literature. The second part explores a body of data which has been adduced in support of a theory of structural preferences --- one that is inconsistent with the present claim. I show how the current proposal can be specified to account for the available data, and moreover to predict where structural preference theories will go wrong. The third part is a theoretical investigation of how well the proposed architecture can be realized using current conceptions of linguistic competence. In it, I present a parsing algorithm and a meaning-based ambiguity resolution method. "
cmp-lg-9406031,arxiv.org/abs/cmp-lg/9406031,20/6/94,A Psycholinguistically Motivated Parser for CCG,0,0,0," A Psycholinguistically Motivated Parser for CCG Considering the speed in which humans resolve syntactic ambiguity, and the overwhelming evidence that syntactic ambiguity is resolved through selection of the analysis whose interpretation is the most `sensible', one comes to the conclusion that interpretation, hence parsing take place incrementally, just about every word. Considerations of parsimony in the theory of the syntactic processor lead one to explore the simplest of parsers: one which represents only analyses as defined by the grammar and no other information. Toward this aim of a simple, incremental parser I explore the proposal that the competence grammar is a Combinatory Categorial Grammar CCG. I address the problem of the proliferating analyses that stem from CCG's associativity of derivation. My solution involves maintaining only the maximally incremental analysis and, when necessary, computing the maximally right-branching analysis. I use results from the study of rewrite systems to show that this computation is efficient. "
cmp-lg-9406028,arxiv.org/abs/cmp-lg/9406028,20/6/94,Resolution of Syntactic Ambiguity: the Case of New Subjects,0,0,0," Resolution of Syntactic Ambiguity: the Case of New Subjects I review evidence for the claim that syntactic ambiguities are resolved on the basis of the meaning of the competing analyses, not their structure. I identify a collection of ambiguities that do not yet have a meaning-based account and propose one which is based on the interaction of discourse and grammatical function. I provide evidence for my proposal by examining statistical properties of the Penn Treebank of syntactically annotated text. "
cmp-lg-9406030,arxiv.org/abs/cmp-lg/9406030,20/6/94,The complexity of normal form rewrite sequences for Associativity,0,0,0," The complexity of normal form rewrite sequences for Associativity The complexity of a particular term-rewrite system is considered: the rule of associativity (x*y)*z --> x*(y*z). Algorithms and exact calculations are given for the longest and shortest sequences of applications of --> that result in normal form NF. The shortest NF sequence for a term x is always n-drm(x), where n is the number of occurrences of * in x and drm(x) is the depth of the rightmost leaf of x. The longest NF sequence for any term is of length n(n-1)/2. "
cmp-lg-9406032,arxiv.org/abs/cmp-lg/9406032,21/6/94,Anytime Algorithms for Speech Parsing?,0,0,0," Anytime Algorithms for Speech Parsing? This paper discusses to which extent the concept of ``anytime algorithms'' can be applied to parsing algorithms with feature unification. We first try to give a more precise definition of what an anytime algorithm is. We arque that parsing algorithms have to be classified as contract algorithms as opposed to (truly) interruptible algorithms. With the restriction that the transaction being active at the time an interrupt is issued has to be completed before the interrupt can be executed, it is possible to provide a parser with limited anytime behavior, which is in fact being realized in our research prototype. "
cmp-lg-9406034,arxiv.org/abs/cmp-lg/9406034,22/6/94,Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French,0,0,0," Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French This paper presents a statistical decision procedure for lexical ambiguity resolution. The algorithm exploits both local syntactic patterns and more distant collocational evidence, generating an efficient, effective, and highly perspicuous recipe for resolving a given ambiguity. By identifying and utilizing only the single best disambiguating evidence in a target context, the algorithm avoids the problematic complex modeling of statistical dependencies. Although directly applicable to a wide class of ambiguities, the algorithm is described and evaluated in a realistic case study, the problem of restoring missing accents in Spanish and French text. "
cmp-lg-9406037,arxiv.org/abs/cmp-lg/9406037,23/6/94,Multi-Paragraph Segmentation of Expository Text,0,0,0," Multi-Paragraph Segmentation of Expository Text This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts. "
cmp-lg-9406036,arxiv.org/abs/cmp-lg/9406036,23/6/94,Text Analysis Tools in Spoken Language Processing,0,0,0, Text Analysis Tools in Spoken Language Processing This submission contains the postscript of the final version of the slides used in our ACL-94 tutorial. 